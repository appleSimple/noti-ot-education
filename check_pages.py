import json
import os
import re
import time
from dataclasses import dataclass
from typing import Dict, List, Set
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

CONFIG_FILE = "targets.json"
STATE_FILE = "state.json"

BOT_TOKEN = os.environ.get("BOT_TOKEN", "").strip()
CHAT_ID = os.environ.get("CHAT_ID", "").strip()

HEADERS = {
    "User-Agent": "Mozilla/5.0 (notice-watcher; +https://github.com/)"
}

@dataclass
class Item:
    item_id: str   # ëª©ë¡ ê¸€ë²ˆí˜¸(ìˆ«ì)
    title: str
    url: str       # ë”¥ë§í¬ê°€ ìˆìœ¼ë©´ ë”¥ë§í¬, ì—†ìœ¼ë©´ ëª©ë¡ URL

def load_config() -> Dict:
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def load_state() -> Dict[str, Set[str]]:
    if not os.path.exists(STATE_FILE):
        return {}
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            raw = json.load(f)
        return {k: set(map(str, v)) for k, v in raw.items()}
    except Exception:
        return {}

def save_state(state: Dict[str, Set[str]]):
    compact = {k: list(sorted(v, reverse=True))[:3000] for k, v in state.items()}
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(compact, f, ensure_ascii=False, indent=2)

def telegram_send(text: str):
    if not BOT_TOKEN or not CHAT_ID:
        print(f"[í…”ë ˆê·¸ë¨ ì•Œë¦¼ ìŠ¤í‚µ - í™˜ê²½ë³€ìˆ˜ ì—†ìŒ]\n{text}")
        return

    api = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHAT_ID,
        "text": text,
        "disable_web_page_preview": True,
    }
    r = requests.post(api, json=payload, timeout=20)
    r.raise_for_status()

def fetch_html(url: str) -> tuple[str, str]:
    """
    returns: (final_url, html_text)
    - ì¸ì½”ë”© ë³´ì • í¬í•¨
    """
    r = requests.get(url, headers=HEADERS, timeout=25)
    r.raise_for_status()

    # ì¸ì½”ë”© ë³´ì • (íŠ¹íˆ EUC-KR/CP949 ì‚¬ì´íŠ¸)
    if not r.encoding or (r.encoding.lower() in ["iso-8859-1", "latin-1"]):
        r.encoding = r.apparent_encoding or r.encoding

    return r.url, r.text

def parse_html_list_number_id(target_url: str, latest_n: int) -> List[Item]:
    """
    ëª©ë¡ì—ì„œ ê¸€ë²ˆí˜¸(ìˆ«ì)ë¥¼ item_idë¡œ ì‚¬ìš©.
    ì „í˜•ì ì¸ í…Œì´ë¸” ëª©ë¡:
      <tr>
        <td>376</td>
        <td><a ...>ì œëª©</a></td>
        ...
      </tr>

    URL:
    - a[href]ê°€ ì‹¤ë§í¬ë©´ urljoiní•´ì„œ ì‚¬ìš©
    - hrefê°€ #/javascriptë©´ onclickì—ì„œ '...' í˜•íƒœ URLì´ ìˆìœ¼ë©´ ì¶”ì¶œ
    - ê·¸ë§ˆì €ë„ ì—†ìœ¼ë©´ target_url(ëª©ë¡) ì‚¬ìš©
    """
    final_url, html = fetch_html(target_url)
    soup = BeautifulSoup(html, "lxml")

    items_by_id: Dict[str, Item] = {}

    # 1) ê°€ì¥ ì•ˆì •ì ì¸ íŒ¨í„´: trì˜ ì²« tdê°€ ìˆ«ì
    for tr in soup.find_all("tr"):
        tds = tr.find_all("td")
        if not tds:
            continue

        no = tds[0].get_text(strip=True)
        if not no.isdigit():
            continue

        a = tr.find("a")
        if not a:
            continue

        title = a.get_text(strip=True)
        if not title:
            continue

        href = (a.get("href") or "").strip()
        onclick = (a.get("onclick") or "").strip()

        full_url = target_url  # fallbackì€ ëª©ë¡
        if href and href not in ["#", "javascript:void(0);", "javascript:void(0)"] and not href.lower().startswith("javascript:"):
            full_url = urljoin(final_url, href)
        else:
            # onclickì— URL ë¬¸ìì—´ì´ ë“¤ì–´ìˆëŠ” ê²½ìš°: 'view.php?...' ë˜ëŠ” "/path/..." ë“±
            url_m = re.search(r"""['"]([^'"]+)['"]""", onclick)
            if url_m:
                full_url = urljoin(final_url, url_m.group(1))

        items_by_id[no] = Item(item_id=no, title=title, url=full_url)

    # 2) í˜¹ì‹œ í…Œì´ë¸” êµ¬ì¡°ê°€ ë‹¬ë¼ì„œ 1)ì´ ë¹„ë©´: aì˜ ë¶€ëª¨ trì—ì„œ ì²« td ìˆ«ì ì°¾ê¸°
    if not items_by_id:
        for a in soup.find_all("a"):
            title = a.get_text(strip=True)
            if not title:
                continue

            tr = a.find_parent("tr")
            if not tr:
                continue
            tds = tr.find_all("td")
            if not tds:
                continue

            no = tds[0].get_text(strip=True)
            if not no.isdigit():
                continue

            href = (a.get("href") or "").strip()
            full_url = urljoin(final_url, href) if href and not href.lower().startswith("javascript:") else target_url
            items_by_id[no] = Item(item_id=no, title=title, url=full_url)

    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def parse_html_link_with_key(target_url: str, latest_n: int) -> List[Item]:
    """
    ë§í¬ì—ì„œ ?Key=ìˆ«ì í˜•íƒœë¡œ ID ì¶”ì¶œ
    ì˜ˆ: /notice/view.asp?Key=47
    """
    final_url, html = fetch_html(target_url)
    soup = BeautifulSoup(html, "lxml")
    
    items_by_id: Dict[str, Item] = {}
    
    for a in soup.find_all("a", href=True):
        href = a.get("href", "").strip()
        title = a.get_text(strip=True)
        
        if not title or len(title) < 3:
            continue
        
        # ?Key=ìˆ«ì íŒ¨í„´ ì°¾ê¸°
        match = re.search(r'[?&]Key=(\d+)', href, re.IGNORECASE)
        if match:
            item_id = match.group(1)
            full_url = urljoin(final_url, href)
            items_by_id[item_id] = Item(item_id=item_id, title=title, url=full_url)
    
    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def parse_html_link_with_path_number(target_url: str, latest_n: int) -> List[Item]:
    """
    ë§í¬ì—ì„œ ê²½ë¡œì— ìˆ«ìê°€ í¬í•¨ëœ í˜•íƒœë¡œ ID ì¶”ì¶œ
    ì˜ˆ: /board2/295
    """
    final_url, html = fetch_html(target_url)
    soup = BeautifulSoup(html, "lxml")
    
    items_by_id: Dict[str, Item] = {}
    
    # target_urlì—ì„œ ê²½ë¡œ íŒ¨í„´ ì¶”ì¶œ (/board2 ë“±)
    from urllib.parse import urlparse
    parsed = urlparse(target_url)
    path_prefix = parsed.path.rstrip('/')
    
    for a in soup.find_all("a", href=True):
        href = a.get("href", "").strip()
        title = a.get_text(strip=True)
        
        if not title or len(title) < 3:
            continue
        
        # ê²½ë¡œ/ìˆ«ì íŒ¨í„´ ì°¾ê¸°
        pattern = rf'{re.escape(path_prefix)}/(\d+)'
        match = re.search(pattern, href)
        if match:
            item_id = match.group(1)
            full_url = urljoin(final_url, href)
            items_by_id[item_id] = Item(item_id=item_id, title=title, url=full_url)
    
    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def parse_html_link_with_num_param(target_url: str, latest_n: int) -> List[Item]:
    """
    ë§í¬ì—ì„œ ?num=ìˆ«ì í˜•íƒœë¡œ ID ì¶”ì¶œ
    ì˜ˆ: ?num=3043&start=0&...
    """
    final_url, html = fetch_html(target_url)
    soup = BeautifulSoup(html, "lxml")
    
    items_by_id: Dict[str, Item] = {}
    
    for a in soup.find_all("a", href=True):
        href = a.get("href", "").strip()
        title = a.get_text(strip=True)
        
        if not title or len(title) < 3:
            continue
        
        # ?num=ìˆ«ì ë˜ëŠ” &num=ìˆ«ì íŒ¨í„´ ì°¾ê¸°
        match = re.search(r'[?&]num=(\d+)', href, re.IGNORECASE)
        if match:
            item_id = match.group(1)
            full_url = urljoin(final_url, href)
            items_by_id[item_id] = Item(item_id=item_id, title=title, url=full_url)
    
    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def run_target(target: Dict, state: Dict[str, Set[str]]):
    name = str(target.get("name", "unknown"))
    url = target["url"]
    ttype = target.get("type", "html_list_number_id")
    latest_n = int(target.get("latest_n", 30))

    seen = state.get(name, set())

    # íƒ€ì…ì— ë§ëŠ” íŒŒì„œ ì„ íƒ
    if ttype == "html_list_number_id":
        items = parse_html_list_number_id(url, latest_n)
    elif ttype == "html_link_with_key":
        items = parse_html_link_with_key(url, latest_n)
    elif ttype == "html_link_with_path_number":
        items = parse_html_link_with_path_number(url, latest_n)
    elif ttype == "html_link_with_num_param":
        items = parse_html_link_with_num_param(url, latest_n)
    else:
        raise ValueError(f"Unsupported target type: {ttype}")

    print(f"[{name}] fetched={len(items)} first5={[ (it.item_id, it.title) for it in items[:5] ]}")

    # í¬ë¡¤ë§ì€ ì„±ê³µí–ˆì§€ë§Œ ê¸€ì„ í•˜ë‚˜ë„ ëª» ì°¾ì€ ê²½ìš° ê²½ê³ 
    if not items:
        warn_msg = f"âš ï¸ íŒŒì‹± ê²½ê³  ({name})\n- ê¸€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n- HTML êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- URL: {url}"
        print(warn_msg)
        telegram_send(warn_msg)
        return

    new_items = [it for it in items if it.item_id not in seen]
    if not new_items:
        print(f"[{name}] No new items.")
        return

    # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì•Œë¦¼ ë³´ë‚´ê¸°
    new_items.sort(key=lambda it: int(it.item_id))

    for it in new_items:
        msg = f"ğŸ†• ìƒˆ ê¸€ ({name})\n- {it.title}\n- {it.url}"
        telegram_send(msg)
        print(f"[{name}] Sent: {it.item_id} {it.title}")
        seen.add(it.item_id)
        time.sleep(0.7)

    state[name] = seen

def main():
    config = load_config()
    targets = config.get("targets", [])
    if not targets:
        raise RuntimeError("targets.jsonì— targetsê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    state = load_state()

    for target in targets:
        try:
            run_target(target, state)
        except Exception as e:
            err_msg = f"âš ï¸ í¬ë¡¤ëŸ¬ ì˜¤ë¥˜ ({target.get('name','unknown')})\n- {type(e).__name__}: {e}"
            print(err_msg)
            telegram_send(err_msg)

    save_state(state)

if __name__ == "__main__":
    main()